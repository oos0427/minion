{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 종류 (계속)\n",
    "\n",
    "## 언어 모델의 종류\n",
    "1. **순방향 언어 모델**\n",
    "   - 문장의 앞에서 뒤로 학습\n",
    "   - 예: GPT\n",
    "\n",
    "2. **역방향 언어 모델**\n",
    "   - 문장의 뒤에서 앞으로 학습\n",
    "   - 예: ELMO\n",
    "\n",
    "3. **마스크 언어 모델**\n",
    "   - 학습 대상 문장에 빈칸을 만들고, 해당 빈칸에 적절한 단어를 분류 모델로 학습\n",
    "   - 예: BERT 등\n",
    "\n",
    "4. **스킵-그램 모델** (현재 잘 사용되지 않음)\n",
    "   - 특정 단어의 앞뒤 범위를 정해, 이 범위 내에서 적절한 단어를 분류 모델로 학습\n",
    "   - 예: Word2Vec\n",
    "\n",
    "5. **Sequence-to-Sequence 모델**\n",
    "   - 특정 속성을 지닌 시퀀스를 다른 속성의 시퀀스로 변환하는 작업\n",
    "   - 예: Transformer (기계 번역 등의 Sequence-to-Sequence 과제를 수행)\n",
    "\n",
    "---\n",
    "\n",
    "## 용어 설명\n",
    "- **Sequence**: 단어 등의 나열, 연속된 데이터, 리스트\n",
    "\n",
    "---\n",
    "\n",
    "## 인코더와 디코더\n",
    "1. **인코더 (Encoder)**\n",
    "   - 소스 시퀀스의 정보를 압축하여 디코더로 전달\n",
    "\n",
    "2. **디코더 (Decoder)**\n",
    "   - 인코더의 출력 데이터를 기반으로 타깃 시퀀스를 생성\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 트랜스포머\n",
    "\n",
    "## 트랜스포머 구조\n",
    "- **인코더의 입력**: 소스 시퀀스\n",
    "- **디코더의 입력**: 타깃 시퀀스 일부\n",
    "- **트랜스포머 학습**: 입력이 주어졌을 때 정답에 해당하는 단어의 확률 값을 높이는 방식으로 수행\n",
    "\n",
    "---\n",
    "\n",
    "### 주요 구성 요소\n",
    "1. **인코더**\n",
    "   - 입력된 소스 시퀀스를 처리\n",
    "   - 위치 정보를 포함하여 입력 데이터를 인코딩\n",
    "   \n",
    "2. **디코더**\n",
    "   - 타깃 시퀀스 일부와 이전 출력 값을 기반으로 다음 단어를 생성\n",
    "\n",
    "---\n",
    "\n",
    "### 예시: \"I went to the cafe\"\n",
    "- 단어별로 위치 정보를 기반으로 인코딩\n",
    "- 디코더는 주어진 타깃 시퀀스를 참고하여 단어 생성 확률을 예측\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "쿼리 : 알고자 하는 질문 ex) 어재 카페 갔었어 거기 사람 많더라<br>\n",
    "키 : 입력된 단어 어재, 갔었어, 카페, 거기, 사람, 많더라<br>\n",
    "벨류 : Z카페 계산시 각 키에 곱해지는 값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 셀프 어텐션 (Self-Attention)\n",
    "\n",
    "## 개념\n",
    "- **셀프 어텐션**: 자신에게 수행하는 어텐션 기법\n",
    "- 입력 시퀀스 가운데 태스크 수행에 의미 있는 요소를 위주로 정보 추출\n",
    "- 입력 시퀀스의 각 요소가 다른 요소와 어떻게 상호 작용하는지 모델링\n",
    "\n",
    "---\n",
    "\n",
    "## 주요 특징\n",
    "- **수행 대상**: 입력 시퀀스 전체\n",
    "- **어텐션 계산 방식**: 개별 단어와 전체 입력 시퀀스를 대상으로 어텐션 계산 수행 → 문맥 정보 학습\n",
    "\n",
    "---\n",
    "\n",
    "## 예시\n",
    "문장: \"어제 카페 갔었어 거기 사람 많더라\"\n",
    "- 각 단어 간의 상호작용 및 문맥 정보를 기반으로 중요도를 계산\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 셀프 어텐션 계산 (Self-Attention Calculation)\n",
    "\n",
    "## 개념\n",
    "- **Query**, **Key**, **Value** 3가지 요소가 서로 영향을 주고받는 구조\n",
    "- 입력된 각 단어 벡터는 계산 과정을 거쳐 **Query**, **Key**, **Value**로 변환\n",
    "- **Query**와 **Key**의 행렬곱 연산으로 단어 사이의 관계를 합이 1인 확률 값으로 도출\n",
    "- 위 연산 결과와 **Value** 벡터를 곱하고 가중합 계산\n",
    "\n",
    "---\n",
    "\n",
    "## 주요 연산 과정\n",
    "1. 각 단어가 Query, Key, Value 벡터로 변환\n",
    "2. Query와 Key 간 유사도 계산 (행렬곱 연산)\n",
    "3. Softmax를 통해 확률값으로 변환\n",
    "4. Value 벡터와 가중합하여 최종 결과 생성\n",
    "\n",
    "---\n",
    "\n",
    "## 연산 예시\n",
    "### 연산식\n",
    "\\[\n",
    "Z_{\\text{카페}} = 0.1 \\times V_{\\text{어제}} + 0.1 \\times V_{\\text{카페}} + 0.2 \\times V_{\\text{갔었어}} + 0.4 \\times V_{\\text{거기}} + 0.1 \\times V_{\\text{사람}} + 0.1 \\times V_{\\text{많더라}}\n",
    "\\]\n",
    "\n",
    "### 계산 흐름\n",
    "- **Query**와 **Key**의 상호작용으로 중요도를 계산\n",
    "- 가중치를 **Value**에 적용하여 문맥 반영\n",
    "\n",
    "---\n",
    "\n",
    "## 시각적 설명\n",
    "- **Query와 Key 연산 결과**: 각 단어의 상관관계를 계산\n",
    "- 예시: \"카페\"를 중심으로 다른 단어와의 상호작용 값 도출\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 멀티 헤드 어텐션 (Multi-Head Attention)\n",
    "\n",
    "## 개념\n",
    "- **멀티 헤드 어텐션**: 셀프 어텐션을 동시에 여러 번 수행하는 방식\n",
    "- 각 헤드는 독자적으로 셀프 어텐션 계산을 수행\n",
    "- **다양한 표현 학습**: 서로 다른 헤드를 통해 다양한 패턴 및 정보를 학습\n",
    "\n",
    "---\n",
    "\n",
    "## 계산 과정\n",
    "1. **각 헤드의 셀프 어텐션 수행**\n",
    "   - 입력 단어 수 × Value 차원 수\n",
    "   - 독립적으로 셀프 어텐션 계산\n",
    "\n",
    "2. **헤드들의 출력 결합**\n",
    "   - 각 헤드의 출력 \\( Z_1, Z_2, \\dots, Z_h \\)를 결합\n",
    "   - 결합된 행렬에 추가 가중치 \\( W_O \\) 적용\n",
    "\n",
    "3. **최종 출력**\n",
    "   - 결과로 얻어진 행렬: 입력 단어 수 × 목표 차원 수\n",
    "\n",
    "---\n",
    "\n",
    "## 주요 연산 단계\n",
    "1. **헤드별 셀프 어텐션 계산**\n",
    "   \\[\n",
    "   Z_i = \\text{Softmax}(Q_i \\cdot K_i^T) \\cdot V_i\n",
    "   \\]\n",
    "\n",
    "2. **헤드 출력 결합**\n",
    "   \\[\n",
    "   Z_{\\text{multi-head}} = [Z_1; Z_2; \\dots; Z_h] \\cdot W_O\n",
    "   \\]\n",
    "\n",
    "---\n",
    "\n",
    "## 결과 설명\n",
    "- **다양한 헤드별 분석**: 각 헤드가 입력 데이터에서 다른 패턴 및 정보를 학습\n",
    "- **최종 가중합**: 여러 헤드의 출력을 통합하여 모델의 목표 차원 수에 맞는 결과 생성\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT와 GPT\n",
    "\n",
    "## GPT (Generative Pre-trained Transformer)\n",
    "- **역할**: 문장 생성\n",
    "- **특징**:\n",
    "  - 다음 단어가 무엇인지 맞추는 과정\n",
    "  - 문장을 왼쪽에서 오른쪽으로 순차적으로 계산 (단방향)\n",
    "  - 트랜스포머에서 **인코더를 제외**하고 디코더만 사용\n",
    "\n",
    "### 예시\n",
    "- 문장: \"어제 카페 갔었어 거기 사람\"\n",
    "  - GPT는 **다음 단어**를 예측: \"많더라\"\n",
    "\n",
    "---\n",
    "\n",
    "## BERT (Bidirectional Encoder Representations from Transformers)\n",
    "- **역할**: 문장의 의미 추출\n",
    "- **특징**:\n",
    "  - 빈칸에 어떤 단어가 적절한지 맞추는 과정\n",
    "  - 빈칸 앞뒤 문맥을 모두 검토 (양방향)\n",
    "  - 트랜스포머에서 **디코더를 제외**하고 인코더만 사용\n",
    "\n",
    "### 예시\n",
    "- 문장: \"어제 카페 갔었어 ___ 많더라\"\n",
    "  - BERT는 빈칸에 적합한 단어를 예측: \"사람\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
